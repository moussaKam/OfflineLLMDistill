dataset:
  name_or_path: "./data/teacher_data.json"
  split: "train"
  # num_samples: none
models:
  student: "Qwen/Qwen2.5-0.5B-Instruct"
  teacher: "Qwen/Qwen2.5-72B-Instruct"
  dtype: "bfloat16"

tokenizer:
  max_length: 2048

training:
  output_dir: "./results_distillation"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  save_strategy: "epoch"
  logging_steps: 50
  learning_rate: 0.00002
  weight_decay: 0.05
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  resume_from_checkpoint: null
  remove_unused_columns: false
  gradient_checkpointing: true

distillation:
  # temperature: 2.0
  alpha: 1
  top_k: 20
  inference_outputs_dir: "./data/inference_outputs"
  do_generate_inference_outputs: true


model_config:
  use_flash_attention: true

vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.95

train_on_completion_only: true
response_template: "\n<|im_start|>assistant\n"